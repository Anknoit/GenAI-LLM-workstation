{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e380659f",
   "metadata": {},
   "source": [
    "## Tokenization Example - nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f6dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection\n",
      "of Telecom, AI, ML, IoT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection\n",
    "of Telecom, AI, ML, IoT.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1f7493",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize \u001b[38;5;66;03m#as in sentence (sent)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# corpus to sentences\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sentence \u001b[38;5;241m=\u001b[39m sent_tokenize(corpus)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize #as in sentence (sent)\n",
    "\n",
    "# corpus to sentences\n",
    "sentence = sent_tokenize(corpus)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21c555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Ankit', '!', 'I', 'ama', 'software', 'developer', ',', 'I', 'work', 'at', 'a', 'Tech', 'company', 'and', 'I', 'am', 'huge', 'AI', 'enthusiast', '.', 'I', 'love', 'Energy', 'industry', 'along', 'with', 'the', 'inersection', 'of', 'Telecom', ',', 'AI', ',', 'ML', ',', 'IoT', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# punctuations also treated as words\n",
    "word_token = word_tokenize(corpus)\n",
    "print(word_token)\n",
    "print(len(word_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4259cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Ankit', '!', 'I', 'ama', 'software', 'developer', ',', 'I', 'work', 'at', 'a', 'Tech', 'company', 'and', 'I', 'am', 'huge', 'AI', 'enthusiast', '.', 'I', 'love', 'Energy', 'industry', 'along', 'with', 'the', 'inersection', 'of', 'Telecom', ',', 'AI', ',', 'ML', ',', 'IoT', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize = wordpunct_tokenize(corpus)\n",
    "print(wordpunct_tokenize)\n",
    "print(len(wordpunct_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb5f6c",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Reducing a word to its root or the origin word, ex. Eat, Eaten, Eating, Eater ----> Stem Word = Eat (The root word) - It removes the prefix suffix additions ina  word such as ly, ing,es etc\n",
    "- PorterStemmer\n",
    "- RegexStemmer\n",
    "- SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f07e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fairli\n",
      "fairness ---> fair\n",
      "getting ---> get\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratul\n",
      "congratulate ---> congratul\n",
      "congrats ---> congrat\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "words = [\"eating\", \"eaten\", \"eater\", \"eat\", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats', 'are', 'is', 'to']\n",
    "for word in words:\n",
    "    print(word, \"--->\", stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3224579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fairly\n",
      "fairness ---> fair\n",
      "getting ---> gett\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratulations\n",
      "congratulate ---> congratulate\n",
      "congrats ---> congrats\n"
     ]
    }
   ],
   "source": [
    "#Regex Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer = RegexpStemmer('ing$|es$|ed$|lly$|ness$')\n",
    "for word in words:\n",
    "    print(word, \"--->\", regex_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fair\n",
      "fairness ---> fair\n",
      "getting ---> get\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratul\n",
      "congratulate ---> congratul\n",
      "congrats ---> congrat\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word, \"--->\", snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccb853",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e61c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat',\n",
       " 'eat',\n",
       " 'eater',\n",
       " 'eat',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fairness',\n",
       " 'get',\n",
       " 'get',\n",
       " 'congratulations',\n",
       " 'congratulate',\n",
       " 'congrats',\n",
       " 'be',\n",
       " 'be',\n",
       " 'to']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Lemmatization - base/dictionary form of word\n",
    "# lemmatize(word: str, pos: str = 'n') â†’ str\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# words = \"eating eaten eater eat fair fairly fairness getting gotten congratulations congratulate congrats are is to\"\n",
    "# text = \"The cats were running faster than the dogs.\"\n",
    "text = [\"eating\", \"eaten\", \"eater\", \"eat\", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats', 'are', 'is', 'to']\n",
    "\n",
    "lemma = [lemmatizer.lemmatize(word, pos='v') for word in text] #using pos = v as most of the words are verb\n",
    "lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0d42",
   "metadata": {},
   "source": [
    "## POS - Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9430935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ankit/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nltk.tag.pos_tag(tokens, tagset=None, lang='eng')\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Taj', 'NOUN'), ('Mahal', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('beautifull', 'ADJ'), ('monument', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"The Taj Mahal is a beautifull monument\"\n",
    "print(pos_tag(word_tokenize(sent), tagset='universal')) #tagset to get universally understood taggin => rather NN we get NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da361969",
   "metadata": {},
   "source": [
    "## NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.chunk.ne_chunk(tagged_tokens, binary=False)\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990b387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " 'buys',\n",
       " '2',\n",
       " 'million',\n",
       " 'bpd',\n",
       " 'Russian',\n",
       " 'oil',\n",
       " 'August',\n",
       " 'The',\n",
       " 'increase',\n",
       " 'Russian',\n",
       " 'flow',\n",
       " 'cost',\n",
       " 'purchases',\n",
       " 'Iraq',\n",
       " ',',\n",
       " 'declined',\n",
       " '730,000',\n",
       " 'bpd',\n",
       " 'August']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = \"\"\" India buys 2 million bpd Russian oil in August\n",
    "The increase in Russian flow was at the cost of purchases from Iraq, which declined to 730,000 bpd in August \"\"\"\n",
    "word = word_tokenize(sent)\n",
    "\n",
    "\n",
    "#Removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in word if word not in stop_words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaad6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', 'NOUN'),\n",
       " ('buys', 'VERB'),\n",
       " ('2', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('bpd', 'NOUN'),\n",
       " ('Russian', 'ADJ'),\n",
       " ('oil', 'NOUN'),\n",
       " ('August', 'NOUN'),\n",
       " ('The', 'DET'),\n",
       " ('increase', 'NOUN'),\n",
       " ('Russian', 'NOUN'),\n",
       " ('flow', 'NOUN'),\n",
       " ('cost', 'NOUN'),\n",
       " ('purchases', 'NOUN'),\n",
       " ('Iraq', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('declined', 'VERB'),\n",
       " ('730,000', 'NUM'),\n",
       " ('bpd', 'NOUN'),\n",
       " ('August', 'NOUN')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tag_pos = pos_tag(words, tagset='universal')\n",
    "tag_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82053d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/ankit/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ankit/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c917ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE India/NOUN)\n",
      "('buys', 'VERB')\n",
      "('2', 'NUM')\n",
      "('million', 'NUM')\n",
      "('bpd', 'NOUN')\n",
      "('Russian', 'ADJ')\n",
      "('oil', 'NOUN')\n",
      "('August', 'NOUN')\n",
      "('The', 'DET')\n",
      "('increase', 'NOUN')\n",
      "('Russian', 'NOUN')\n",
      "('flow', 'NOUN')\n",
      "('cost', 'NOUN')\n",
      "('purchases', 'NOUN')\n",
      "(GPE Iraq/NOUN)\n",
      "(',', '.')\n",
      "('declined', 'VERB')\n",
      "('730,000', 'NUM')\n",
      "('bpd', 'NOUN')\n",
      "('August', 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "ner = ne_chunk(tag_pos)\n",
    "type(ner)\n",
    "# GPE indicates countries, states, cities, and places\n",
    "for i in ner:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da836672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
