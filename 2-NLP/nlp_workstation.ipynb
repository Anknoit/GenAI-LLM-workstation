{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d408e2",
   "metadata": {},
   "source": [
    "## Tokenization Example - nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ca8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection\n",
      "of Telecom, AI, ML, IoT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection\n",
    "of Telecom, AI, ML, IoT.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477d2b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nHi my name is Ankit!', 'I ama software developer, I work at a Tech company and I am huge AI enthusiast.', 'I love Energy industry along with the inersection\\nof Telecom, AI, ML, IoT.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize #as in sentence (sent)\n",
    "\n",
    "# corpus to sentences\n",
    "sentence = sent_tokenize(corpus)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d25529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Ankit', '!', 'I', 'ama', 'software', 'developer', ',', 'I', 'work', 'at', 'a', 'Tech', 'company', 'and', 'I', 'am', 'huge', 'AI', 'enthusiast', '.', 'I', 'love', 'Energy', 'industry', 'along', 'with', 'the', 'inersection', 'of', 'Telecom', ',', 'AI', ',', 'ML', ',', 'IoT', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# punctuations also treated as words\n",
    "word_token = word_tokenize(corpus)\n",
    "print(word_token)\n",
    "print(len(word_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479bd97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Ankit', '!', 'I', 'ama', 'software', 'developer', ',', 'I', 'work', 'at', 'a', 'Tech', 'company', 'and', 'I', 'am', 'huge', 'AI', 'enthusiast', '.', 'I', 'love', 'Energy', 'industry', 'along', 'with', 'the', 'inersection', 'of', 'Telecom', ',', 'AI', ',', 'ML', ',', 'IoT', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize = wordpunct_tokenize(corpus)\n",
    "print(wordpunct_tokenize)\n",
    "print(len(wordpunct_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744089",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Reducing a word to its root or the origin word, ex. Eat, Eaten, Eating, Eater ----> Stem Word = Eat (The root word) - It removes the prefix suffix additions ina  word such as ly, ing,es etc\n",
    "- PorterStemmer\n",
    "- RegexStemmer\n",
    "- SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df073d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fairli\n",
      "fairness ---> fair\n",
      "getting ---> get\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratul\n",
      "congratulate ---> congratul\n",
      "congrats ---> congrat\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "words = [\"eating\", \"eaten\", \"eater\", \"eat\", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats', 'are', 'is', 'to']\n",
    "for word in words:\n",
    "    print(word, \"--->\", stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d045bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fairly\n",
      "fairness ---> fair\n",
      "getting ---> gett\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratulations\n",
      "congratulate ---> congratulate\n",
      "congrats ---> congrats\n"
     ]
    }
   ],
   "source": [
    "#Regex Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer = RegexpStemmer('ing$|es$|ed$|lly$|ness$')\n",
    "for word in words:\n",
    "    print(word, \"--->\", regex_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d9da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fair\n",
      "fairness ---> fair\n",
      "getting ---> get\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratul\n",
      "congratulate ---> congratul\n",
      "congrats ---> congrat\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word, \"--->\", snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fe15a-ef4d-4727-822d-7ff759e969ce",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061a762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat',\n",
       " 'eat',\n",
       " 'eater',\n",
       " 'eat',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fairness',\n",
       " 'get',\n",
       " 'get',\n",
       " 'congratulations',\n",
       " 'congratulate',\n",
       " 'congrats',\n",
       " 'be',\n",
       " 'be',\n",
       " 'to']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatization - base/dictionary form of word\n",
    "# lemmatize(word: str, pos: str = 'n') â†’ str\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# words = \"eating eaten eater eat fair fairly fairness getting gotten congratulations congratulate congrats are is to\"\n",
    "# text = \"The cats were running faster than the dogs.\"\n",
    "text = [\"eating\", \"eaten\", \"eater\", \"eat\", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats', 'are', 'is', 'to']\n",
    "\n",
    "lemma = [lemmatizer.lemmatize(word, pos='v') for word in text] #using pos = v as most of the words are verb\n",
    "lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6be09e-0045-4be3-bd09-94192767fa56",
   "metadata": {},
   "source": [
    "## POS - Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74a95b6e-dcb4-4124-93a2-f7642b7501ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ankit/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.tag.pos_tag(tokens, tagset=None, lang='eng')\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e2347ec-370d-475a-a6e7-22081ff14c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Taj', 'NOUN'), ('Mahal', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('beautifull', 'ADJ'), ('monument', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"The Taj Mahal is a beautifull monument\"\n",
    "print(pos_tag(word_tokenize(sent), tagset='universal')) #tagset to get universally understood taggin => rather NN we get NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964c313-2e55-4c78-bccd-c8d1c3deb5e7",
   "metadata": {},
   "source": [
    "## NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9971d455-48c1-44aa-8d4f-e935ec31d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.chunk.ne_chunk(tagged_tokens, binary=False)\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64ba0c0a-e1f4-4c10-badb-7976d872d895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " 'buys',\n",
       " '2',\n",
       " 'million',\n",
       " 'bpd',\n",
       " 'Russian',\n",
       " 'oil',\n",
       " 'August',\n",
       " 'The',\n",
       " 'increase',\n",
       " 'Russian',\n",
       " 'flow',\n",
       " 'cost',\n",
       " 'purchases',\n",
       " 'Iraq',\n",
       " ',',\n",
       " 'declined',\n",
       " '730,000',\n",
       " 'bpd',\n",
       " 'August']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"\"\" India buys 2 million bpd Russian oil in August\n",
    "The increase in Russian flow was at the cost of purchases from Iraq, which declined to 730,000 bpd in August \"\"\"\n",
    "word = word_tokenize(sent)\n",
    "\n",
    "\n",
    "#Removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in word if word not in stop_words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc21f435-4a53-4717-a65f-4ce19b2c3e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', 'NOUN'),\n",
       " ('buys', 'VERB'),\n",
       " ('2', 'NUM'),\n",
       " ('million', 'NUM'),\n",
       " ('bpd', 'NOUN'),\n",
       " ('Russian', 'ADJ'),\n",
       " ('oil', 'NOUN'),\n",
       " ('August', 'NOUN'),\n",
       " ('The', 'DET'),\n",
       " ('increase', 'NOUN'),\n",
       " ('Russian', 'NOUN'),\n",
       " ('flow', 'NOUN'),\n",
       " ('cost', 'NOUN'),\n",
       " ('purchases', 'NOUN'),\n",
       " ('Iraq', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('declined', 'VERB'),\n",
       " ('730,000', 'NUM'),\n",
       " ('bpd', 'NOUN'),\n",
       " ('August', 'NOUN')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_pos = pos_tag(words, tagset='universal')\n",
    "tag_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98e82b33-c40c-495f-a0d8-99ddc08d1c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/ankit/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ankit/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "150dfb11-c1a1-4be8-b0f2-9f6cd16beacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE India/NOUN)\n",
      "('buys', 'VERB')\n",
      "('2', 'NUM')\n",
      "('million', 'NUM')\n",
      "('bpd', 'NOUN')\n",
      "('Russian', 'ADJ')\n",
      "('oil', 'NOUN')\n",
      "('August', 'NOUN')\n",
      "('The', 'DET')\n",
      "('increase', 'NOUN')\n",
      "('Russian', 'NOUN')\n",
      "('flow', 'NOUN')\n",
      "('cost', 'NOUN')\n",
      "('purchases', 'NOUN')\n",
      "(GPE Iraq/NOUN)\n",
      "(',', '.')\n",
      "('declined', 'VERB')\n",
      "('730,000', 'NUM')\n",
      "('bpd', 'NOUN')\n",
      "('August', 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "ner = ne_chunk(tag_pos)\n",
    "type(ner)\n",
    "# GPE indicates countries, states, cities, and places\n",
    "for i in ner:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3c3fb-c930-4b5d-bbfd-9f606e63d47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
