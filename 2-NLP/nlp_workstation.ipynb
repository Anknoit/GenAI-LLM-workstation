{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d408e2",
   "metadata": {},
   "source": [
    "## Tokenization Example - nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection\n",
      "of Telecom, AI, ML, IoT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection\n",
    "of Telecom, AI, ML, IoT.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d40b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ankit/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477d2b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nHi my name is Ankit!', 'I ama software developer, I work at a Tech company and I am huge AI enthusiast.', 'I love Energy industry along with the inersection\\nof Telecom, AI, ML, IoT.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize #as in sentence (sent)\n",
    "\n",
    "# corpus to sentences\n",
    "sentence = sent_tokenize(corpus)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d25529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Ankit', '!', 'I', 'ama', 'software', 'developer', ',', 'I', 'work', 'at', 'a', 'Tech', 'company', 'and', 'I', 'am', 'huge', 'AI', 'enthusiast', '.', 'I', 'love', 'Energy', 'industry', 'along', 'with', 'the', 'inersection', 'of', 'Telecom', ',', 'AI', ',', 'ML', ',', 'IoT', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# punctuations also treated as words\n",
    "word_token = word_tokenize(corpus)\n",
    "print(word_token)\n",
    "print(len(word_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479bd97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', 'is', 'Ankit', '!', 'I', 'ama', 'software', 'developer', ',', 'I', 'work', 'at', 'a', 'Tech', 'company', 'and', 'I', 'am', 'huge', 'AI', 'enthusiast', '.', 'I', 'love', 'Energy', 'industry', 'along', 'with', 'the', 'inersection', 'of', 'Telecom', ',', 'AI', ',', 'ML', ',', 'IoT', '.']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize = wordpunct_tokenize(corpus)\n",
    "print(wordpunct_tokenize)\n",
    "print(len(wordpunct_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744089",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Reducing a word to its root or the origin word, ex. Eat, Eaten, Eating, Eater ----> Stem Word = Eat (The root word)\n",
    "- PorterStemmer\n",
    "- RegexStemmer\n",
    "- SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df073d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fairli\n",
      "fairness ---> fair\n",
      "getting ---> get\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratul\n",
      "congratulate ---> congratul\n",
      "congrats ---> congrat\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "words = [\"eating\", \"eaten\", \"eater\", \"eat\", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats']\n",
    "for word in words:\n",
    "    print(word, \"--->\", stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d045bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eaten ---> eaten\n",
      "eater ---> eater\n",
      "eat ---> eat\n",
      "fair ---> fair\n",
      "fairly ---> fairly\n",
      "fairness ---> fair\n",
      "getting ---> gett\n",
      "gotten ---> gotten\n",
      "congratulations ---> congratulations\n",
      "congratulate ---> congratulate\n",
      "congrats ---> congrats\n"
     ]
    }
   ],
   "source": [
    "#Regex Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "regex_stemmer = RegexpStemmer('ing$|es$|ed$|lly$|ness$')\n",
    "for word in words:\n",
    "    print(word, \"--->\", regex_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9da87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snowball_stemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m snowball_stemer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(word, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--->\u001b[39m\u001b[38;5;124m\"\u001b[39m, snowball_stemmer\u001b[38;5;241m.\u001b[39mstem(word))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snowball_stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word, \"--->\", snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a762e",
   "metadata": {},
   "source": [
    "## Word Vectorization\n",
    "Converting words, sentences, corpus, and datasets to vector for model training\n",
    "\n",
    "Types of Vectorization methods\n",
    "- Frequency based\n",
    "    - CountVectorizer(BOW) - Bag of words\n",
    "    - TF-IDF - Term Frequency - Inverse Document Frequency\n",
    "- Word Embedding Based\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "    - FastText\n",
    "- Modern, Deep Contextual Model\n",
    "    - BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bf1cc",
   "metadata": {},
   "source": [
    "## Implementing CBOW Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e962d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "context=[]\n",
    "target =[]\n",
    "corpus = [\"The mangos are very sweet and tangy\"]\n",
    "window_size = 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
