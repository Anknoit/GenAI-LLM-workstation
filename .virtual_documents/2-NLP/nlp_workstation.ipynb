


corpus = """
Hi my name is Ankit! I ama software developer, I work at a Tech company and I am huge AI enthusiast. I love Energy industry along with the inersection
of Telecom, AI, ML, IoT.
"""
print(corpus)


from nltk.tokenize import sent_tokenize #as in sentence (sent)

# corpus to sentences
sentence = sent_tokenize(corpus)
print(sentence)


from nltk.tokenize import word_tokenize
# punctuations also treated as words
word_token = word_tokenize(corpus)
print(word_token)
print(len(word_token))


from nltk.tokenize import wordpunct_tokenize
wordpunct_tokenize = wordpunct_tokenize(corpus)
print(wordpunct_tokenize)
print(len(wordpunct_tokenize))





# Porter Stemming
from nltk.stem import PorterStemmer
stemming = PorterStemmer()
words = ["eating", "eaten", "eater", "eat", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats', 'are', 'is', 'to']
for word in words:
    print(word, "--->", stemming.stem(word))


#Regex Stemming
from nltk.stem import RegexpStemmer
regex_stemmer = RegexpStemmer('ing$|es$|ed$|lly$|ness$')
for word in words:
    print(word, "--->", regex_stemmer.stem(word))


# Snowball Stemmer
from nltk.stem import SnowballStemmer

snowball_stemmer = SnowballStemmer('english')
for word in words:
    print(word, "--->", snowball_stemmer.stem(word))





## Lemmatization - base/dictionary form of word
# lemmatize(word: str, pos: str = 'n') â†’ str
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
# words = "eating eaten eater eat fair fairly fairness getting gotten congratulations congratulate congrats are is to"
# text = "The cats were running faster than the dogs."
text = ["eating", "eaten", "eater", "eat", 'fair', 'fairly', 'fairness', 'getting', 'gotten', 'congratulations', 'congratulate', 'congrats', 'are', 'is', 'to']

lemma = [lemmatizer.lemmatize(word, pos='v') for word in text] #using pos = v as most of the words are verb
lemma





# nltk.tag.pos_tag(tokens, tagset=None, lang='eng')
import nltk
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
nltk.download('universal_tagset')


sent = "The Taj Mahal is a beautifull monument"
print(pos_tag(word_tokenize(sent), tagset='universal')) #tagset to get universally understood taggin => rather NN we get NOUN





import nltk
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from nltk.tokenize import word_tokenize


sent = """ India buys 2 million bpd Russian oil in August
The increase in Russian flow was at the cost of purchases from Iraq, which declined to 730,000 bpd in August """
words = word_tokenize(sent)
words


tag_pos = pos_tag(words)
tag_pos


nltk.download('maxent_ne_chunker_tab')
nltk.download('words')



ner = ne_chunk(tag_pos)
ner.draw()



